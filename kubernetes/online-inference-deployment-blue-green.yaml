apiVersion: apps/v1
kind: Deployment
metadata:
  name: online-inference-deploy
  labels:
    app: online-inference
spec:
  replicas: 3
  selector:
    matchLabels: 
      app: online-inference
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 3
  template:
    metadata:
      name: online-inference-deploy
      labels:
        app: online-inference
    spec:
      containers:
        - image: docker-tutorial
          name: online-inference
          ports:
            - containerPort: 80
          resources:
            requests:
              memory: "128Mi"
              cpu: "250m"
            limits:
              memory: "256Mi"
              cpu: "500m"
